{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3509433225.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [6]\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install feature-engine\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# IBM_HR_Attrition_Rate_Analytics\n",
    "# EDA Analysis\n",
    "\n",
    "pip install feature-engine\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from feature_engine.encoding import DecisionTreeEncoder, CountFrequencyEncoder, WoEEncoder, RareLabelEncoder\n",
    "\n",
    "# To set display options\n",
    "pd.options.display.max_columns = 300\n",
    "pd.options.display.max_rows = 300\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Data Loading \n",
    "df = pd.read_csv(\"../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n",
    "df.head()\n",
    "# Data View\n",
    "df.info()\n",
    "df.dtypes\n",
    "df.isna().sum()\n",
    "df.duplicated().sum()\n",
    "df.shape\n",
    "numeric_columns = list(df.select_dtypes(include=np.number).columns)\n",
    "categorical_columns = list(df.select_dtypes(include=\"object\").columns)\n",
    "df[numeric_columns].describe()\n",
    "df[categorical_columns].describe()\n",
    "\n",
    "#Univariate Plot\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize = (20,10))\n",
    "\n",
    "ax = np.ravel(ax)\n",
    "for i in range(len(categorical_columns)):\n",
    "    sns.countplot(data = df, x = categorical_columns[i], ax = ax[i], palette=\"rocket\", order = df[categorical_columns[i]].value_counts().index)\n",
    "    ax[i].set_xticklabels(labels = df[categorical_columns[i]].unique(), rotation=45, ha='right')\n",
    "ax = np.reshape(ax, (3, 3))\n",
    "plt.tight_layout()\n",
    "\n",
    "\"\"\"Over18 only have one Value\n",
    " Attrition is heavily unbalanced\n",
    " Most of the employees travel rarely\n",
    " Most of the employees work overtimes\n",
    " There are more females than males\"\"\"\n",
    "\n",
    "def make_distplot(df, col, ax):\n",
    "    sns.distplot(df[col], ax = ax)\n",
    "    ax.axvline(df[col].mean(), linestyle = '--', color = \"red\")\n",
    "    ax.axvline(df[col].median(), linestyle = '--', color = \"green\")\n",
    "\n",
    "fig, ax = plt.subplots(6,5, figsize = (20,15))\n",
    "\n",
    "ax = np.ravel(ax)\n",
    "for i in range(len(numeric_columns)):\n",
    "    make_distplot(df, numeric_columns[i], ax[i])\n",
    "for i in range(len(numeric_columns), 6*5):\n",
    "    ax[i].axis(\"off\")\n",
    "ax = np.reshape(ax, (6, 5))\n",
    "plt.tight_layout()\n",
    "\n",
    "\"\"\"There are employees which work for over 10 years without a promotion which is quite strange\n",
    " The performance rating of the employees isn't the best with ~ 3.0 as a median \"\"\"\n",
    "\n",
    "#Bivariate plots\n",
    "\n",
    "corr = df[numeric_columns].corr()\n",
    "\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "cmap = sns.color_palette(\"icefire\", as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr, mask = mask, cmap = cmap, annot=False, fmt= '.2f', vmin=-1, vmax=1, square = True, linewidth=2, cbar_kws={\"shrink\": 0.7}, ax=ax)\n",
    "\n",
    "# correlations among the numerical columns\n",
    "\"\"\"High correlation between Age and JobLevel, MonthlyIncome and TotalWorkingYears\n",
    "High correlation between JobLevel and MonthlyIncome\n",
    "High correlation between TotalWorkingYears and YearsSinceLastPromotion -> The longer you are working, the higher is the probability that you reached your \"limit\" in terms of promotion\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize = (20,10))\n",
    "\n",
    "ax = np.ravel(ax)\n",
    "for i in range(len(categorical_columns)):\n",
    "    sns.countplot(data = df, x = categorical_columns[i], hue=\"Attrition\", ax = ax[i], palette=\"rocket\", order = df[categorical_columns[i]].value_counts().index)\n",
    "    ax[i].set_xticklabels(labels = df[categorical_columns[i]].unique(), rotation=45, ha='right')\n",
    "ax = np.reshape(ax, (3, 3))\n",
    "plt.tight_layout()\n",
    "\n",
    "\"\"\"Nearly half of the Sales Representatives went from the company away\n",
    "Most of the people were traveling rarely and Sales is the department with the highest attrition (but also with the most employees overall)\"\"\"\n",
    "\n",
    "df.groupby(\"Gender\")[\"YearsSinceLastPromotion\"].mean()\n",
    "\n",
    "df.groupby(\"Gender\")[\"PerformanceRating\"].mean()\n",
    "\n",
    "Females are waiting a bit longer for a promotion than men, although having a better performance rating\n",
    "\n",
    "# Data Preprocessing\n",
    "\n",
    "# Drop unnecessary columns\n",
    "\n",
    "df = df.drop([\"Over18\", \"EmployeeCount\", \"StandardHours\"], axis=1)\n",
    "X = df.loc[:, df.columns != \"Attrition\"].copy()\n",
    "y = df.loc[:, \"Attrition\"].copy()\n",
    "\n",
    "# Label Encoding for Target Attrition\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "y = pd.DataFrame(y, index=df.index, columns=[\"Attrition\"])\n",
    "categorical_cols_features = list(X.select_dtypes(include=\"object\").columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, shuffle = True)\n",
    "\n",
    "# Rare Label Encoding\n",
    "\n",
    "# Grouping Labels which have a low amount of observations. They will go into a new category named \"Rare\" to prevent overfitting\n",
    "\n",
    "def rare_encoding(X_train, X_test):\n",
    "    rare_encoder = RareLabelEncoder(\n",
    "                    tol = 0.05, # Threshold: Labels with a frequency < tol will be grouped\n",
    "                    n_categories = 4,  # minimum categories required for encoding\n",
    "                    variables = categorical_cols_features)\n",
    "    \n",
    "    rare_encoder.fit(X_train)\n",
    "    X_train_rare_encoded = rare_encoder.transform(X_train)\n",
    "    X_test_rare_encoded = rare_encoder.transform(X_test)\n",
    "    \n",
    "    return X_train_rare_encoded, X_test_rare_encoded\n",
    "\n",
    "X_train_rare, X_test_rare = rare_encoding(X_train, X_test)\n",
    "\n",
    "X_train[\"EducationField\"].value_counts(normalize=True)\n",
    "\n",
    "X_train[X_train[\"EducationField\"] == \"Human Resources\"][\"EducationField\"].head()\n",
    "\n",
    "X_train_rare[(X_train_rare[\"EducationField\"] == 'Rare')][\"EducationField\"]\n",
    "\n",
    "# Now that we have done Rare Encoding Need to look at the other techniques other than Ordinal Encoding or One Hot Encoding\n",
    "\n",
    "\"\"\"First Need to write our function for the three different techniques:\n",
    "Frequency Encoding\n",
    "Decision Tree Encoding\n",
    "Weight of Evidence (WoE) Encoding\"\"\"\n",
    "\n",
    "def encoding(X_train, X_test, y_train, method=None):\n",
    "    if method:\n",
    "        if method == \"frequency\":\n",
    "            freq_encoder = CountFrequencyEncoder(encoding_method='frequency',variables=categorical_cols_features)\n",
    "            freq_encoder.fit(X_train)\n",
    "            X_train_encoded = freq_encoder.transform(X_train)\n",
    "            X_test_encoded = freq_encoder.transform(X_test)\n",
    "            \n",
    "        elif method == \"decisiontree\":\n",
    "            dt_encoder = DecisionTreeEncoder(\n",
    "            variables=categorical_cols_features,\n",
    "            encoding_method='arbitrary', \n",
    "            cv=3,\n",
    "            scoring='f1',\n",
    "            param_grid={'max_depth': [i for i in range (3, 7)]}, # Grid search parameters\n",
    "            regression=False) \n",
    "            \n",
    "            dt_encoder.fit(X_train, y_train)\n",
    "            X_train_encoded = dt_encoder.transform(X_train)\n",
    "            X_test_encoded = dt_encoder.transform(X_test)\n",
    "            \n",
    "        elif method == \"woe\":\n",
    "            woe_encoder = WoEEncoder(variables=categorical_cols_features)\n",
    "            woe_encoder.fit(X_train, y_train.iloc[:, 0]) \n",
    "            X_train_encoded = woe_encoder.transform(X_train)\n",
    "            X_test_encoded = woe_encoder.transform(X_test)\n",
    "            \n",
    "        return X_train_encoded, X_test_encoded\n",
    "    \n",
    "    # Frequency Encoding\n",
    "    \n",
    "    \"\"\"Categories will be replaced by the percentage of observations per category. \\\n",
    "        E.g. Category Life Sciences have a proportion of 0.41 so Life Sciences will be replaced by 0.41\"\"\"\n",
    "\n",
    "X_train_freq_encoded, X_test_freq_encoded = encoding(X_train, X_test, y_train, method=\"frequency\")\n",
    "\n",
    "X_train_freq_encoded[categorical_cols_features].head()\n",
    "\n",
    "# Decision Tree Encoding\n",
    "\"\"\"The Decision Tree Encoder encodes categorical variables with predictions of a decision tree model. The encoder fits \n",
    "a Decision Tree with a single feature and the target,and then replaces the original categories by the predictions\"\"\"\n",
    "\n",
    "X_train_dt_encoded, X_dt_freq_encoded = encoding(X_train, X_test, y_train, method=\"decisiontree\")\n",
    "X_train_dt_encoded[categorical_cols_features].head()\n",
    "\n",
    "# WoE Encoding\n",
    "\n",
    "\"\"\"WoE is calculated by taking the natural logarithm of division of % of non-events and % of events\n",
    "\n",
    "WOE = In(% of non-events âž— % of events)\"\"\"\n",
    "\n",
    "y_train.iloc[:, 0]\n",
    "\n",
    "X_train_woe_encoded, X_test_woe_encoded = encoding(X_train, X_test, y_train, method=\"woe\")\n",
    "\n",
    "X_train_woe_encoded.head()\n",
    "\n",
    "#Evaluate the three different techniques\n",
    "\n",
    "scoring = [\"roc_auc\", \"f1\", \"balanced_accuracy\"]\n",
    "\n",
    "# With Frequency Encoding\n",
    "\n",
    "model_1 = LogisticRegression(random_state = 42, solver='liblinear')\n",
    "\n",
    "scores_1 = cross_validate(model_1, X_train_freq_encoded, y_train.values.ravel(), scoring = scoring, cv=3)\n",
    "\n",
    "scores_1[\"test_f1\"].mean()\n",
    "\n",
    "# With Decision Tree Encoding\n",
    "\n",
    "model_2 = LogisticRegression(random_state = 42, solver='liblinear')\n",
    "\n",
    "scores_2 = cross_validate(model_2, X_train_dt_encoded, y_train.values.ravel(), scoring = scoring, cv=3)\n",
    "\n",
    "scores_2[\"test_f1\"].mean()\n",
    "\n",
    "# With WoE Encoding\n",
    "\n",
    "model_3 = LogisticRegression(random_state = 42, solver='liblinear')\n",
    "\n",
    "scores_3 = cross_validate(model_3, X_train_woe_encoded, y_train.values.ravel(), scoring = scoring, cv=3)\n",
    "\n",
    "scores_3[\"test_f1\"].mean()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9528d81b706951917a41ce6fc0a833588d4279002574da204bcc41f57c33bfaf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
